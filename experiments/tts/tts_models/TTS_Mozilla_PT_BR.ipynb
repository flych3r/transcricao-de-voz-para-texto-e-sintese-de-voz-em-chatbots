{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TTS_Mozilla_PT_BR.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9hrazsFpbNV6","colab_type":"text"},"source":["## Cloning WaveRNN repository"]},{"cell_type":"code","metadata":{"id":"a5OaJZA8KMdz","colab_type":"code","colab":{}},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0APzqIEZrrlM","colab_type":"code","colab":{}},"source":["import os\n","\n","!git clone https://github.com/erogol/WaveRNN.git\n","os.chdir('WaveRNN')\n","!git checkout 12c8744\n","!pip install -r requirements.txt\n","#download weights\n","!rm saver-wavernn.zip\n","#!wget https://www.dropbox.com/s/jp5stdlbvfqw30u/checkpoint-wavernn-finetunnig-tts-portuguese-corpus-544400.zip?dl=0 -O saver-wavernn.zip\n","!wget https://www.dropbox.com/s/4a60kt3detcw3r6/checkpoint-wavernn-finetunnig-tts-portuguese-corpus-560900.zip?dl=0 -O saver-wavernn.zip\n","!ls  \n","!unzip saver-wavernn.zip \n","\n","os.chdir('..')\n","!git clone https://github.com/Edresson/TTS -b TTS-Portuguese\n","#os.chdir('TTS')\n","#!pip install -r requirements.txt\n","#os.chdir('..')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DYNcHFiHsoxe","colab_type":"code","colab":{}},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TRy8HBcSGfoq","colab_type":"code","colab":{}},"source":["!cat TTS/requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yvb0pX3WY6MN","colab_type":"code","colab":{}},"source":["# !python -m pip install -r TTS/requirements.txt\n","!pip install lws Unidecode tensorboardX Pillow phonemizer pydub\n","!apt-get install espeak"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MpYNgqrZcJKn","colab_type":"text"},"source":["## Import modules"]},{"cell_type":"code","metadata":{"id":"4KZA4b_CbMqx","colab_type":"code","colab":{}},"source":["%load_ext autoreload\n","%autoreload 2\n","import os\n","import sys\n","import io\n","import torch \n","import time\n","import numpy as np\n","from collections import OrderedDict\n","\n","TTS_PATH = \"../content/TTS\"\n","WAVERNN_PATH =\"../content/WaveRNN\"\n","# add libraries into environment\n","sys.path.append(TTS_PATH) # set this if TTS is not installed globally\n","sys.path.append(WAVERNN_PATH) # set this if TTS is not installed globally\n","\n","import matplotlib.pyplot as plt\n","# pylab as plt\n","# %pylab inline\n","\n","from matplotlib import rcParams \n","rcParams[\"figure.figsize\"] = (16,5)\n","sys.path.append('')\n","\n","import librosa\n","import librosa.display\n","\n","from TTS.models.tacotron import Tacotron \n","from TTS.layers import *\n","from TTS.utils.data import *\n","from TTS.utils.audio import AudioProcessor\n","from TTS.utils.generic_utils import load_config\n","from TTS.utils.text import text_to_sequence, phoneme_to_sequence, sequence_to_phoneme\n","from TTS.utils.text.symbols import symbols, phonemes\n","\n","import IPython\n","from IPython.display import Audio\n","from TTS.utils import *\n","\n","\n","from TTS.utils.visual import visualize\n","\n","from pydub import AudioSegment\n","# from matplotlib import pylab as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w6Krn8k1inC_","colab_type":"text"},"source":["## Download Weights"]},{"cell_type":"code","metadata":{"id":"PiYHf3lKhi9z","colab_type":"code","colab":{}},"source":["#!wget -c -q --show-progress -O ./TTS-TL-saver.zip https://www.dropbox.com/s/b88peo1triqvvpe/checkpoint_255k-tts-portuguese-with-phonemes.zip?dl=0\n","#!wget -c -q --show-progress -O ./TTS-TL-saver.zip  https://www.dropbox.com/s/szrhtl75njx9ic6/TTS-checkpoint-phonemizer-wavernn-362600.zip?dl=0\n","!wget -c -q --show-progress -O ./TTS-TL-saver.zip https://www.dropbox.com/s/91etfwt4tvzjqyz/TTS-checkpoint-phonemizer-wavernn-381000.zip?dl=0\n","!ls\n","!rm config.json\n","!unzip TTS-TL-saver.zip\n","! mv checkpoint_381000.pth.tar checkpoint.pth.tar\n","#!wget -c -q --show-progress -O checkpoint_255200.pth.tar https://www.dropbox.com/s/phwp3bk64dlhx8u/checkpoint_255200.pth.tar?dl=0\n","#! mv checkpoint_255200.pth.tar checkpoint.pth.tar"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XNhRrGp-r1sm","colab_type":"text"},"source":["## Inference Functions"]},{"cell_type":"code","metadata":{"id":"lWm8ObhEHko5","colab_type":"code","colab":{}},"source":["def plot_alignment_with_text(alignment,text, info=None):\n","    fig, ax = plt.subplots(figsize=(16, 10))\n","    im = ax.imshow(\n","        alignment.T, aspect='auto', origin='lower', interpolation=None)\n","    fig.colorbar(im, ax=ax)\n","    xlabel = 'Decoder timestep'\n","    if info is not None:\n","        xlabel += '\\n\\n' + info\n","    plt.xlabel(xlabel)\n","    plt.ylabel('Encoder timestep')\n","    plt.yticks(range(len(text)), list(text))\n","    plt.tight_layout()\n","    return fig\n","  \n","\n","from TTS.utils.synthesis import visualize\n","\n","def synthesis(m, s, CONFIG, use_cuda, ap,language=None,WaveRNN=False):\n","    \"\"\" Given the text, synthesising the audio \"\"\"\n","    if language is None:\n","      language=CONFIG.phoneme_language\n","    text_cleaner = [CONFIG.text_cleaner]\n","    # print(phoneme_to_sequence(s, text_cleaner))\n","    # print(sequence_to_phoneme(phoneme_to_sequence(s, text_cleaner)))\n","    if CONFIG.use_phonemes:\n","        seq = np.asarray(\n","            phoneme_to_sequence(s, text_cleaner, language),\n","            dtype=np.int32)\n","    else:\n","        seq = np.asarray(text_to_sequence(s, text_cleaner), dtype=np.int32)\n","    chars_var = torch.from_numpy(seq).unsqueeze(0)\n","    if use_cuda:\n","        chars_var = chars_var.cuda()\n","    mel_spec, linear_spec, alignments, stop_tokens = m.forward(\n","        chars_var.long()\n","    )\n","    linear_spec = linear_spec[0].data.cpu().numpy()\n","    mel_spec = mel_spec[0].data.cpu().numpy()\n","    alignment = alignments[0].cpu().data.numpy()\n","    if not WaveRNN:\n","      wav = ap.inv_spectrogram(linear_spec.T)\n","      wav = wav[:ap.find_endpoint(wav)]\n","    else:\n","      wav = wavernn.generate(torch.FloatTensor(mel_spec.T).unsqueeze(0).cuda(), batched=True, target=11000, overlap=550)\n","    return wav, alignment, linear_spec, mel_spec, stop_tokens\n","\n","def tts_griffin_lim(model, text, CONFIG, use_cuda, ap, figures=True,name='figure',language=None, display=True):\n","    t_1 = time.time()\n","    waveform, alignment, spectrogram, mel_spec, stop_tokens = synthesis(model, text, CONFIG, use_cuda, ap, language=language, WaveRNN=False) \n","    if display:\n","        print(\" >  Run-time with Griffin lim: {}\".format(time.time() - t_1))\n","        print(\"Vocoder Griffin-Lim :\") \n","        IPython.display.display(Audio(waveform, rate=ap.sample_rate))  \n","    if figures:\n","        fig = plot_alignment_with_text(alignment,text)\n","        visualize(alignment, spectrogram, stop_tokens,text,250, CONFIG,mel_spec) \n","        fig.savefig(os.path.join(OUT_FOLDER,'alig_'+name+'.png'))\n","    \n","    return alignment, spectrogram, stop_tokens, waveform\n","\n","def tts_wave_rnn(model, text, CONFIG, use_cuda, audio_processor, figures=True,name='figure',language=None, display=True):\n","    t_1 = time.time()\n","    waveform, alignment, spectrogram, mel_spec, stop_tokens = synthesis(model, text, CONFIG, use_cuda, audio_processor, language=language, WaveRNN=True) \n","    if display:\n","        print(\" >  Run-time with WaveRNN: {}\".format(time.time() - t_1))\n","        print(\"Vocoder Griffin-Lim :\") \n","        IPython.display.display(Audio(waveform, rate=audio_processor.sample_rate))  \n","    if figures:\n","        fig = plot_alignment_with_text(alignment,text)\n","        visualize(alignment, spectrogram, stop_tokens,text,250, CONFIG,mel_spec) \n","        fig.savefig(os.path.join(OUT_FOLDER,'alig_'+name+'.png'))\n","    \n","    return alignment, spectrogram, stop_tokens, waveform   \n","  \n","# Set constants\n","\n","MODEL_PATH = 'checkpoint.pth.tar'\n","CONFIG_PATH =  'TTS/config.json'\n","OUT_FOLDER = 'samples/'\n","try:\n","  os.mkdir(OUT_FOLDER)\n","except:\n","  pass\n","\n","CONFIG = load_config(CONFIG_PATH)\n","\n","use_cuda = torch.cuda.is_available()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dV6cXXlfi72r","colab_type":"text"},"source":["## Restore  TTS Model"]},{"cell_type":"code","metadata":{"id":"iyflP9IcGfR1","colab_type":"code","colab":{}},"source":["VOCODER_MODEL_PATH = \"WaveRNN/saver.pth.tar\"\n","VOCODER_CONFIG_PATH = \"WaveRNN/config_16K.json\"\n","VOCODER_CONFIG = load_config(VOCODER_CONFIG_PATH)\n","\n","# load the model\n","ap2 = AudioProcessor(**VOCODER_CONFIG.audio)\n","ap = AudioProcessor(**CONFIG.audio)\n","\n","num_chars = len(phonemes) if CONFIG.use_phonemes else len(symbols)\n","! mv checkpoint_381200.pth.tar checkpoint.pth.tar\n","model= Tacotron(num_chars, CONFIG.embedding_size, ap.num_freq, ap.num_mels, CONFIG.r, CONFIG.memory_size)\n","\n","# load model state\n","if use_cuda:\n","    cp = torch.load(MODEL_PATH)\n","else:\n","    cp = torch.load(MODEL_PATH, map_location=lambda storage, loc: storage)\n","\n","# load the model\n","model.load_state_dict(cp['model'])\n","if use_cuda:\n","    model.cuda()\n","model.eval()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R_708yqvnyaq","colab_type":"text"},"source":["## Restore WaveRnn model"]},{"cell_type":"code","metadata":{"id":"J0ju52L9sBhx","colab_type":"code","colab":{}},"source":["#from utils.generic_utils import load_config\n","from WaveRNN.models.wavernn import Model\n","\n","bits = 10\n","\n","wavernn = Model(\n","    rnn_dims=512,\n","    fc_dims=512,\n","    mode=VOCODER_CONFIG.mode,\n","    mulaw=VOCODER_CONFIG.mulaw,\n","    pad=VOCODER_CONFIG.pad,\n","    use_aux_net=VOCODER_CONFIG.use_aux_net,\n","    use_upsample_net=VOCODER_CONFIG.use_upsample_net,\n","    upsample_factors=VOCODER_CONFIG.upsample_factors,\n","    feat_dims=80,\n","    compute_dims=128,\n","    res_out_dims=128,\n","    res_blocks=10,\n","    hop_length=ap2.hop_length,\n","    sample_rate=ap2.sample_rate,\n",")\n","        \n","check = torch.load(VOCODER_MODEL_PATH, map_location=torch.device('cuda' if use_cuda else 'cpu'))\n","wavernn.load_state_dict(check['model'])\n","if use_cuda:\n","    wavernn.cuda()\n","wavernn.eval();\n","print(check['step'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d_lETp7JHUtP","colab_type":"text"},"source":["## Synthesize test sentences"]},{"cell_type":"code","metadata":{"id":"7L2RAMGDRnna","colab_type":"code","colab":{}},"source":["import pydub\n","\n","def process_audio(audio):\n","    return sum(pydub.silence.split_on_silence(audio, silence_thresh=-36)).set_sample_width(2).set_channels(1).set_frame_rate(22500)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EfrsWP-gQuaf","colab_type":"code","colab":{}},"source":["import wave\n","import contextlib\n","\n","def get_audio_length(file_path):\n","    if file_path.endswith('.wav'):\n","        with contextlib.closing(wave.open(file_path,'r')) as f:\n","            frames = f.getnframes()\n","            rate = f.getframerate()\n","            duration = frames / float(rate)\n","            return duration\n","    if file_path.endswith('.mp3'):\n","        audio = MP3(file_path)\n","        return audio.info.length\n","    raise Exception('Unsuported file format. File must be wav or mp3')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VbCJrmN1KiSH","colab_type":"code","colab":{}},"source":["!unzip sentences.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oI8ezOZnKkNV","colab_type":"code","colab":{}},"source":["import glob\n","\n","sentences_dict = dict()\n","for fp in glob.glob('*.txt'):\n","    with open(fp) as f:\n","        sentences = f.readlines()\n","    sentences_dict[fp] = [sent.strip() for sent in sentences][:-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g-sHHznPHRPu","colab_type":"code","colab":{}},"source":["# test_sentences =[\n","#         # \"O capital de uma empresa depende de sua produção\",\n","#         # \"Se não fosse ela tudo teria sido melhor, ou talvez não\",\n","#         # \"A principal personagem no filme é uma gueixa\",\n","#         # \"Espere seu amigo em casa\",\n","#         # \"A juventude tinha que revolucionar a escola\",\n","#         # \"A cantora terá quatro meses para ensaiar seu canto\",\n","# ]\n","\n","import textwrap\n","import io\n","\n","wave_rnn = False\n","\n","if wave_rnn:\n","    model.decoder.max_decoder_steps = 500\n","    br = 50\n","else:\n","    model.decoder.max_decoder_steps = 300\n","    br = 30\n","\n","with open('times.txt', 'w') as f:\n","    print('fp, inf_time, sent_length, audio_length', file=f)\n","\n","for n, sentences in sentences_dict.items():\n","    n = n.split('_')[1].split('.')[0].upper()\n","    for idx, sentence in enumerate(sentences):\n","        if idx == 2:\n","            break\n","        print('\\n'.join(textwrap.wrap(sentence, width=br)))\n","        print('#' * br)\n","        start = time.time()\n","        wavs = []\n","        for frase in textwrap.wrap(sentence, width=br, break_long_words=False):\n","            f = io.BytesIO()\n","            if wave_rnn:\n","                align, spec, stop_tokens, wav = tts_wave_rnn(model, frase, CONFIG, use_cuda, ap2, figures=False, name=str(idx+1), display=False)\n","                ap2.save_wav(wav, f)\n","            else:\n","                align, spec, stop_tokens, wav = tts_griffin_lim(model, ' ' + frase + ' ', CONFIG, use_cuda, ap, figures=False, name=str(idx+1), display=False)\n","                ap.save_wav(wav, f)\n","            wavs.append(f)\n","        wavs = [AudioSegment.from_file_using_temporary_files(w) for w in wavs]\n","        wav = sum(wavs)\n","        wav = process_audio(wav)\n","        fp = '{}_{}'.format(n, idx)\n","        wav.export(\"{}.wav\".format(fp), format=\"wav\")\n","        end = time.time()\n","        print('\\n', end - start, 'segundos')\n","        with open('times.txt', 'a') as f:\n","            print('{}, {}, {}, {}'.format(fp, end - start, len(sentence), get_audio_length('{}.wav'.format(fp))), file=f)\n","        IPython.display.display(wav)\n","        print('*' * 80)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1bIjQXg0O09-","colab_type":"code","colab":{}},"source":["# from notify import send\n","# send('Finished Griffin-Lin run')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQZOjpd8TE2w","colab_type":"code","colab":{}},"source":["# %%bash\n","\n","# mkdir audios\n","# rm audios/*.wav\n","# mv *.wav audios\n","# zip -r audios_griffin_cpu.zip audios times.txt sentences.txt\n","# cp audios_griffin_gpu.zip '/content/drive/My Drive/TCC_data'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"etSnGhD6gGwU","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}
